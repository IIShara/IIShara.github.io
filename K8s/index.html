<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Ilgiz Sharafeev" /><link rel="canonical" href="https://IIShara.github.io/K8s/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Kubernetes - Личная документация</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Kubernetes";
        var mkdocs_page_input_path = "K8s.md";
        var mkdocs_page_url = "/K8s/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/shell.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Личная документация
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Главная</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Заметки</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../Markdown/">Синтаксис Markdown</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Git/">Основные команды Git</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Инструкции</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../Keycloak/">Keycloak</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Prometheus/">Prometheus</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Ansible/">Ansible</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../GitLab/">GitLab</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Kubernetes</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#_1">Типовой состав ПО рабочего узла</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#kubeadm">Развертывание с помощью утилиты kubeadm.</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_2">Схема виртуального стенда</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_3">Предварительная настройка узлов кластера</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#_4">Настройка имен узлов кластера</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hosts">Настройка файла hosts</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_5">Установка вспомогательных пакетов</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#linux-kubernetes">Предварительная подготовка Linux для использования Kubernetes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#kubeadm-kubelet-kubectl">Установка kubeadm, kubelet и kubectl</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#selinux-permissive">Установите SELinux в permissive режим:</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubernetes-yum">Добавьте Kubernetes yum репозиторий.</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubelet-kubeadm-kubectl">Установите kubelet, kubeadm и kubectl:</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubelet-kubeadm">(Необязательно) Включите службу kubelet перед запуском kubeadm:</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_6">Установка контейнерного движка</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#containerd">Установка containerd</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#containerd_1">Проверка доступности сокета containerd</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#containerd_2">Проверка возможности запуска контейнеров с помощью containerd</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#kubernetes_1">Развёртывание Kubernetes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#_7">Настройка балансировщика нагрузки</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#keepalived">Настройка демона keepalived</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#haproxy">Настройка демона haproxy</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_8">Установка управляющих узлов кластера</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#_9">Установка первого управляющего узла</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#_10">Установка последующих управляющих узлов</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_11">Установка рабочих узлов кластера</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#kubetl">Настройка kubeсtl</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#_12">Установка сетевого плагина</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#kubernetes_2">Проверка работы кластера Kubernetes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#kubernetes_3">Тестовые запуски "подов" в Kubernetes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#1">Тест 1. Запуск "пода" в интерактивном режиме.</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#_13">Вспомогательные команды:</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2-nginx">Тест 2. Запуск NGINX</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Linux</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" >Мониторинг</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Top/">Top</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../vmstat/">Vmstat</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Сборка пакетов</a>
    <ul>
                <li class="toctree-l2"><a class="" href="../RPM rebild.md">Пересборка RPM пакета NGINX</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Личная документация</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Инструкции</li>
      <li class="breadcrumb-item active">Kubernetes</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="kubernetes">Kubernetes</h1>
<h2 id="_1">Типовой состав ПО рабочего узла</h2>
<ol>
<li>Служебные компоненты Kubernetes: агент управления узлом kubelet, узловой прокси kube-proxy</li>
<li>Сетевой плагин (Container Network Interface, CNI plugin).</li>
<li>Контейнерный движок: cri-o, containerd или Docker + cri-dockerd plugin.
Рабочие нагрузки (workloads), то есть сами контейнеры, из-за которых все и затевалось. Однако, здесь важно уточнить один существенный момент — минимальной единицей управления рабочей нагрузкой в Kuberbetes является "под" (pod), состоящий из одного (как правило) или нескольких контейнеров.</li>
</ol>
<p><img alt="" src="../img/k8s/image1.png" /></p>
<h2 id="kubeadm">Развертывание с помощью утилиты kubeadm.</h2>
<h2 id="_2">Схема виртуального стенда</h2>
<p>Согласно официальной документации, к машинам, на которых разворачивается Kubernetes, выдвигаются следующие требования:
- 2+ GB ОЗУ;
- 2+ процессорных ядра;
- Linux хост с отключенным файлом подкачки (swap);</p>
<p>При разворачивании кластера на нескольких узлах, согласно тем же документам, накладываются дополнительные требования:
- полная сетевая связанность узлов;
- на каждом узле должны быть уникальные:
    - имена узлов (проверка с помощью команды "hostname"),
    - MAC-адреса (проверка с помощью команды "ip link"),
    - параметр product_uuid, являющийся уникальным идентификатором виртуальной машины (проверка с помощью команды "cat /sys/class/dmi/id/product_uui").</p>
<p>В ходе экспериментов выяснились, что дополнительно к этому узлы должны иметь статические IP-адреса и зарегистрированные DNS имена, что требуется для автоматического выпуска сертификатов во время работы kubeadm.</p>
<p>Минимальное количество рабочих узлов для схемы с резервированием – 2. Логичное требование: один сломался другой на замену. Минимальное количество управляющих узлов для схемы с резервирование – 3. Данное странное требование продиктовано официальной документацией: в Kubernetes должно быть нечетное количество управляющих улов. Минимальное число нечетное число для обеспечения избыточности — 3.</p>
<p>Виртуальные машины будут работать под управлением ОС CentOS 9 x64, установленной с минимальным количеством пакетов. Все необходимое будем явно доставлять.</p>
<h2 id="_3">Предварительная настройка узлов кластера</h2>
<h3 id="_4">Настройка имен узлов кластера</h3>
<p>На узле node1 выполним команду:</p>
<pre><code>hostnamectl set-hostname node1.internal
</code></pre>
<p>На узле node2 выполним команду:</p>
<pre><code>hostnamectl set-hostname node2.internal
</code></pre>
<p>На узле node3 выполним команду:</p>
<pre><code>hostnamectl set-hostname node3.internal
</code></pre>
<p>На узле node4 выполним команду:</p>
<pre><code>hostnamectl set-hostname node4.internal
</code></pre>
<p>На узле node5 выполним команду:</p>
<pre><code>hostnamectl set-hostname node5.internal
</code></pre>
<h3 id="hosts">Настройка файла hosts</h3>
<p>Поскольку мы не используем DNS-сервер, то для разрешения важных для нас DNS-имен настроим файлы hosts на всех узлах кластера.</p>
<p>На всех узлах выполним следующую команду:</p>
<pre><code class="language-bash">cat &gt; /etc/hosts &lt;&lt;EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

# Cluster nodes
192.168.0.61    node1.internal
192.168.0.62    node2.internal
192.168.0.63    node3.internal
192.168.0.64    node4.internal
192.168.0.65    node5.internal
EOF
</code></pre>
<h3 id="_5">Установка вспомогательных пакетов</h3>
<p>На всех узлах выполним команду:</p>
<pre><code>dnf install -y curl wget gnupg sudo iptables
</code></pre>
<p>На узле node1 выполним команду:</p>
<pre><code>dnf install -y tmux
</code></pre>
<p>На узлах node1, node2, node3 выполним команду:</p>
<pre><code>dnf install -y keepalived haproxy
</code></pre>
<h3 id="linux-kubernetes">Предварительная подготовка Linux для использования Kubernetes</h3>
<p>Согласно официальной документации, для работы Kubernetes необходимо разрешить маршрутизацию IPv4 трафика, настроить возможность iptables видеть трафик, передаваемый в режиме моста, а также отключить файлы подкачки.</p>
<p>На всех узлах выполним команды:</p>
<pre><code># Настройка автозагрузки и запуск модуля ядра br_netfilter и overlay
cat &lt;&lt;EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

# Разрешение маршрутизации IP-трафика
echo -e &quot;net.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1&quot; &gt; /etc/sysctl.d/10-k8s.conf
sysctl -f /etc/sysctl.d/10-k8s.conf

# Отключение файла подкачки
swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab
</code></pre>
<p><strong>Проверка корректности настройки</strong></p>
<p>Чтобы убедиться, что все требуемые параметры настроены правильно, рекомендуется перезагрузить виртуальную машину.</p>
<p>Для проверки автоматической загрузки модулей br_netfilter и overlay выполним команды:</p>
<pre><code>lsmod | grep br_netfilter
lsmod | grep overlay

## Ожидаемый результат должен быть следующим (цифры могут отличаться):
# br_netfilter           32768  0
# bridge                258048  1 br_netfilter
# overlay               147456  0
</code></pre>
<p>Для проверки успешности изменения настроек в параметрах сетевого стека выполним команду:</p>
<pre><code>sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward

## Ожидаемый результат:
# net.bridge.bridge-nf-call-iptables = 1
# net.bridge.bridge-nf-call-ip6tables = 1
# net.ipv4.ip_forward = 1
</code></pre>
<p>Для проверки отключения файла подкачки выполним команду:</p>
<pre><code>swapon -s

## Ожидаемый вывод команды – пустой. Она ничего не должна отобразить.
</code></pre>
<h2 id="kubeadm-kubelet-kubectl">Установка kubeadm, kubelet и kubectl</h2>
<p>kubectl – основная утилита командной строки для управления кластером Kubernetes, kubeadm – утилита для развертывания кластера Kubernetes. Установка данных утилит осуществляется в соответствии с официальным руководством.</p>
<p>На всех узлах выполним следующие команды:</p>
<h3 id="selinux-permissive">Установите SELinux в permissive режим:</h3>
<pre><code># Set SELinux in permissive mode (effectively disabling it)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
</code></pre>
<h3 id="kubernetes-yum">Добавьте Kubernetes yum репозиторий.</h3>
<p>Параметр exclude в определении репозитория гарантирует, что пакеты, связанные с Kubernetes, не будут обновлены при запуске yum update поскольку существует специальная процедура, которой необходимо следовать для обновления Kubernetes.</p>
<pre><code># This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo
cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
</code></pre>
<h3 id="kubelet-kubeadm-kubectl">Установите kubelet, kubeadm и kubectl:</h3>
<pre><code>sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
</code></pre>
<h3 id="kubelet-kubeadm">(Необязательно) Включите службу kubelet перед запуском kubeadm:</h3>
<pre><code>sudo systemctl enable --now kubelet
</code></pre>
<h2 id="_6">Установка контейнерного движка</h2>
<h3 id="containerd">Установка containerd</h3>
<p>На всех узлах выполним команды:</p>
<pre><code class="language-bash"># Установка containerd
wget https://github.com/containerd/containerd/releases/download/v1.7.0/containerd-1.7.0-linux-amd64.tar.gz
tar Cxzvf /usr/local containerd-1.7.0-linux-amd64.tar.gz
rm containerd-1.7.0-linux-amd64.tar.gz

# Создание конфигурации по умолчанию для containerd
mkdir /etc/containerd/
containerd config default &gt; /etc/containerd/config.toml

# Настройка cgroup драйвера
sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

# Установка systemd сервиса для containerd
wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
mv containerd.service /etc/systemd/system/

# Установка компонента runc
wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64
install -m 755 runc.amd64 /usr/local/sbin/runc
rm runc.amd64

# Установка сетевых плагинов:
wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz
mkdir -p /opt/cni/bin
tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.2.0.tgz
rm cni-plugins-linux-amd64-v1.2.0.tgz

# Запуск сервиса containerd
systemctl daemon-reload
systemctl enable --now containerd
</code></pre>
<h3 id="containerd_1">Проверка доступности сокета containerd</h3>
<p>На всех узлах выполним команду:</p>
<pre><code>crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock version

## Ожидаемый результат:
# Version:  0.1.0
# RuntimeName:  containerd
# RuntimeVersion:  v1.7.0
# RuntimeApiVersion:  v1
</code></pre>
<h3 id="containerd_2">Проверка возможности запуска контейнеров с помощью containerd</h3>
<p>На всех узлах выполним команды:</p>
<pre><code>ctr images pull docker.io/library/hello-world:latest
ctr run docker.io/library/hello-world:latest hello-world

## Ожидаемый результат:
# …
# Hello from Docker!
# This message shows that your installation appears to be working correctly.
# …
</code></pre>
<h2 id="kubernetes_1">Развёртывание Kubernetes</h2>
<h3 id="_7">Настройка балансировщика нагрузки</h3>
<p>Для создания виртуального IP адреса и перераспределение нагрузки между управляющими узлами будем использовать комбинацию двух демонов: keepalived и haproxy. Этих ребят мы с вами установили ранее на узлы: node1, node2, node3.</p>
<p>Реализуемый нами балансировщик нагрузки будет работать следующим образом (Рисунок 12):
1. Демон keepalived обеспечит функционирование виртуального IP-адреса и его привязку к одному из управляющих узлов. Виртуальный IP будет вторым адресом на сетевом интерфейсе узла. Если данный узел откажет, то keepalived обнаружит это и перекинет виртуальный IP-адрес на другой доступный узел.
2. Поступающие на управляющий узел запросы будут обрабатываться демоном haproxy, который, выполняя роль реверс-прокси (reverse proxy), будет поочередно (round robin) пересылать их на API сервера управляющих узлов Kubernetes.</p>
<p>При развертывания балансировщика нагрузки будем использовать следующие сетевые настройки:
- в качестве виртуального адреса будет использоваться 192.168.0.66;
- связанное с виртуальным адресом DNS имя: k8s-cp.internal;
- TCP порт для доступа к системе управления: 8888;
- в качестве бэкендов будут использоваться порты 6443 на управляющих узлах, другими словами, 192.168.0.61:6443, 192.168.0.62:6443, 192.168.0.63:6443;</p>
<h4 id="keepalived">Настройка демона keepalived</h4>
<p>На узлах node1, node2, node3 создадим и отредактируем основной конфигурационный файл демона keepalived /etc/keepalived/keepalived.conf следующим образом:</p>
<pre><code># File: /etc/keepalived/keepalived.conf

global_defs {
    enable_script_security
    script_user nobody
}

vrrp_script check_apiserver {
  script &quot;/etc/keepalived/check_apiserver.sh&quot;
  interval 3
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth1
    virtual_router_id 5
    priority 100
    advert_int 1
    nopreempt
    authentication {
        auth_type PASS
        auth_pass ZqSj#f1G
    }
    virtual_ipaddress {
        192.168.0.66
    }
    track_script {
        check_apiserver
    }
}

</code></pre>
<p>На узлах node1, node2, node3 создадим и отредактируем скрипт /etc/keepalived/check_apiserver.sh, предназначенный для проверки доступности серверов.</p>
<pre><code>#!/bin/sh
# File: /etc/keepalived/check_apiserver.sh

APISERVER_VIP=192.168.0.66
APISERVER_DEST_PORT=8888
PROTO=http

errorExit() {
    echo &quot;*** $*&quot; 1&gt;&amp;2
    exit 1
}

curl --silent --max-time 2 --insecure ${PROTO}://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit &quot;Error GET ${PROTO}://localhost:${APISERVER_DEST_PORT}/&quot;
if ip addr | grep -q ${APISERVER_VIP}; then
    curl --silent --max-time 2 --insecure ${PROTO}://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit &quot;Error GET ${PROTO}://${APISERVER_VIP}:${APISERVER_DEST_PORT}/&quot;
fi
</code></pre>
<p>На узлах node1, node2, node3 установим атрибут, разрешающий исполнение скрипта, и запустим демона keepalived.</p>
<pre><code>chmod +x /etc/keepalived/check_apiserver.sh
systemctl enable keepalived
systemctl start keepalived
</code></pre>
<h4 id="haproxy">Настройка демона haproxy</h4>
<p>На узлах node1, node2, node3 отредактируем основной конфигурационный файл демона haproxy /etc/haproxy/haproxy.cfg следующим образом:</p>
<pre><code># File: /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    log /dev/log local0
    log /dev/log local1 notice
    daemon

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 1
    timeout http-request    10s
    timeout queue           20s
    timeout connect         5s
    timeout client          20s
    timeout server          20s
    timeout http-keep-alive 10s
    timeout check           10s

#---------------------------------------------------------------------
# apiserver frontend which proxys to the control plane nodes
#---------------------------------------------------------------------
frontend apiserver
    bind *:8888
    mode tcp
    option tcplog
    default_backend apiserver

#---------------------------------------------------------------------
# round robin balancing for apiserver
#---------------------------------------------------------------------
backend apiserver
    option httpchk GET /healthz
    http-check expect status 200
    mode tcp
    option ssl-hello-chk
    balance     roundrobin
        server node1 192.168.0.61:6443 check
        server node2 192.168.0.62:6443 check
        server node3 192.168.0.63:6443 check
</code></pre>
<p>На узлах node1, node2, node3 запустим демона haproxy, выполнив команды:</p>
<pre><code>systemctl enable haproxy
systemctl restart haproxy
</code></pre>
<p><em><strong>Примечание.</strong> Демон будет ругаться, что не обнаружены backend сервера. Это нормально, так как Kubernetes API еще не запущен.</em></p>
<h3 id="_8">Установка управляющих узлов кластера</h3>
<p>Установка производится по официальной документации, выбран режим stacked control plane.</p>
<h4 id="_9">Установка первого управляющего узла</h4>
<p>На node1</p>
<pre><code>kubeadm init \
               --pod-network-cidr=10.244.0.0/16 \
               --control-plane-endpoint &quot;192.168.0.66:8888&quot; \
               --upload-certs
</code></pre>
<p><em><strong>Примечание 1.</strong> --pod-network-cidr=10.244.0.0/16 выбрано для упрощения дальнейшей установки сетевого плагина flannel.</em></p>
<p><em><strong>Примечание 2.</strong> --control-plane-endpoint «192.168.0.66:8888» указывает на виртуальный IP адрес, используемый для управления кластером.</em></p>
<p>По окончанию процедуры должна появиться строка для добавления управляющих узлов в кластер.</p>
<pre><code>You can now join any number of control-plane nodes running the following command on each as root:

  kubeadm join 192.168.0.66:8888 --token xfz55x.ycyp3vd4wpo0icy5 \
        --discovery-token-ca-cert-hash sha256:cbb9250c075a6ca47e78ddb9ba1c27dee11707a937d660a54a447b5d0d46bb45 \
        --control-plane --certificate-key 9731c7251ae7841698337e07742131ac8cac0f22a9b25a37a068352d3432d384
</code></pre>
<p>Кроме указанной строки, будет показана строка для добавления рабочих узлов в кластер.</p>
<pre><code>kubeadm join 192.168.0.66:8888 --token xfz55x.ycyp3vd4wpo0icy5 \
        --discovery-token-ca-cert-hash sha256:cbb9250c075a6ca47e78ddb9ba1c27dee11707a937d660a54a447b5d0d46bb45
</code></pre>
<h4 id="_10">Установка последующих управляющих узлов</h4>
<p>На node2, node3
Используем строку подключения, полученную после создания первого управляющего узла кластера.</p>
<p><em><strong>Внимание!</strong> В строке содержится конфиденциальная информация. Сертификаты для подключения будут автоматически удалены после 2-х часов с момента первичной инициализации кластера.</em></p>
<p>В случае успешного добавления узла среди вывода kubeadm должна быть строка:</p>
<pre><code>…
This node has joined the cluster and a new control plane instance was created:
…
</code></pre>
<h3 id="_11">Установка рабочих узлов кластера</h3>
<p>На узлах node4, node5 запускаем команду добавления рабочих узлов, полученную при установке первого управляющего узла.</p>
<p><em><strong>Примечание.</strong> Если по каким-то причинам вы ее потеряли, то на любом узле кластера введите команду «kubeadm token create --print-join-command», и она отобразится снова.</em></p>
<h3 id="kubetl">Настройка kubeсtl</h3>
<p>На узлах node1, node2, node3 выполним команду:</p>
<pre><code>echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt; /etc/environment
export KUBECONFIG=/etc/kubernetes/admin.conf
</code></pre>
<h3 id="_12">Установка сетевого плагина</h3>
<p>На node1 запускаем команду:</p>
<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<p>Отказоустойчивый кластер Kubernetes готов.</p>
<h2 id="kubernetes_2">Проверка работы кластера Kubernetes</h2>
<p>Приведенные ниже проверки можно запускать на любом узле кластера, на котором настроена работа kubectl.</p>
<p>Проверка включения узлов в кластер:</p>
<pre><code>kubectl get nodes

## Ожидаемый ответ:
#NAME             STATUS   ROLES           AGE   VERSION
#node1.internal   Ready    control-plane   23h   v1.32.2
#node2.internal   Ready    control-plane   23h   v1.32.2
#node3.internal   Ready    control-plane   12h   v1.32.2
#node4.internal   Ready    &lt;none&gt;          12h   v1.32.2
#node5.internal   Ready    &lt;none&gt;          12h   v1.32.2

</code></pre>
<p>При использовании kubeadm все управляющее ПО кластера работает на нем в виде "подов". Очень важно, чтобы все "поды" работали правильным образом, и не было их циклических перезапусков (restarts).</p>
<p>Проверить состояние "подов" можно с помощью команды:</p>
<pre><code>kubectl get pods -A

# Ожидаемый результат
# NAMESPACE      NAME                                     READY   STATUS    RESTARTS      AGE
# default        busybox                                  1/1     Running   1 (12h ago)   12h
# default        nginx-app-65fd5796b9-hx76v               1/1     Running   0             12h
# kube-flannel   kube-flannel-ds-5jcdp                    1/1     Running   1 (12h ago)   12h
# kube-flannel   kube-flannel-ds-cvg28                    1/1     Running   0             12h
# kube-flannel   kube-flannel-ds-j6h6c                    1/1     Running   0             12h
# kube-flannel   kube-flannel-ds-jgpgh                    1/1     Running   0             12h
# kube-flannel   kube-flannel-ds-pwwwq                    1/1     Running   0             12h
# kube-system    coredns-668d6bf9bc-f2xpl                 1/1     Running   0             24h
# kube-system    coredns-668d6bf9bc-x542h                 1/1     Running   0             24h
# kube-system    etcd-node1.internal                      1/1     Running   0             24h
# kube-system    etcd-node2.internal                      1/1     Running   0             23h
# kube-system    etcd-node3.internal                      1/1     Running   0             12h
# kube-system    kube-apiserver-node1.internal            1/1     Running   0             24h
# kube-system    kube-apiserver-node2.internal            1/1     Running   0             23h
# kube-system    kube-apiserver-node3.internal            1/1     Running   0             12h
# kube-system    kube-controller-manager-node1.internal   1/1     Running   0             24h
# kube-system    kube-controller-manager-node2.internal   1/1     Running   0             23h
# kube-system    kube-controller-manager-node3.internal   1/1     Running   0             12h
# kube-system    kube-proxy-g77kv                         1/1     Running   0             24h
# kube-system    kube-proxy-gqr98                         1/1     Running   0             12h
# kube-system    kube-proxy-l4hw4                         1/1     Running   0             12h
# kube-system    kube-proxy-q2gtn                         1/1     Running   0             23h
# kube-system    kube-proxy-q65lb                         1/1     Running   0             12h
# kube-system    kube-scheduler-node1.internal            1/1     Running   0             24h
# kube-system    kube-scheduler-node2.internal            1/1     Running   0             23h
# kube-system    kube-scheduler-node3.internal            1/1     Running   0             12h
</code></pre>
<h2 id="kubernetes_3">Тестовые запуски "подов" в Kubernetes</h2>
<h3 id="1">Тест 1. Запуск "пода" в интерактивном режиме.</h3>
<pre><code>kubectl run -i --tty busybox --image=busybox -- sh

## Ожидаемый результат:
# If you don't see a command prompt, try pressing enter.
# / #
# / #
</code></pre>
<h4 id="_13">Вспомогательные команды:</h4>
<p>Переподключение к "поду" при выходе из интерактивного режима:</p>
<pre><code>kubectl attach busybox -i
</code></pre>
<p>Удаление "пода":</p>
<pre><code>kubectl delete pod busybox
</code></pre>
<h3 id="2-nginx">Тест 2. Запуск NGINX</h3>
<p>Тест заключается в запуске Web-сервера nginx и обращения к нему после этого.</p>
<pre><code>kubectl create deployment nginx-app --image=nginx
kubectl expose deployment nginx-app --type=NodePort --port=80 --external-ip=10.10.10.10
sleep 5s
curl http://10.10.10.10

## Ожидаемый результат
# &lt;!DOCTYPE html&gt;
# &lt;html&gt;
# &lt;head&gt;
# &lt;title&gt;Welcome to nginx!&lt;/title&gt;
# ...
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../GitLab/" class="btn btn-neutral float-left" title="GitLab"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Top/" class="btn btn-neutral float-right" title="Top">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../GitLab/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Top/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
